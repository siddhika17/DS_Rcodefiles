{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d3a2634-9da4-4325-b4ea-52bbc92fb51e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\adhil\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk import download\n",
    "download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9f6bdf6-3d3f-4d4a-b549-c9a2b7889ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('blogs.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cac9eef6-815d-472b-9b91-bea72319db26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Exploration:\n",
      "                                                Data       Labels\n",
      "0  Path: cantaloupe.srv.cs.cmu.edu!magnesium.club...  alt.atheism\n",
      "1  Newsgroups: alt.atheism\\nPath: cantaloupe.srv....  alt.atheism\n",
      "2  Path: cantaloupe.srv.cs.cmu.edu!das-news.harva...  alt.atheism\n",
      "3  Path: cantaloupe.srv.cs.cmu.edu!magnesium.club...  alt.atheism\n",
      "4  Xref: cantaloupe.srv.cs.cmu.edu alt.atheism:53...  alt.atheism\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2000 entries, 0 to 1999\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   Data    2000 non-null   object\n",
      " 1   Labels  2000 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 31.4+ KB\n",
      "None\n",
      "                                                     Data       Labels\n",
      "count                                                2000         2000\n",
      "unique                                               2000           20\n",
      "top     Path: cantaloupe.srv.cs.cmu.edu!magnesium.club...  alt.atheism\n",
      "freq                                                    1          100\n"
     ]
    }
   ],
   "source": [
    "# Data Exploration and Preprocessing\n",
    "print(\"Data Exploration:\")\n",
    "print(df.head())\n",
    "print(df.info())\n",
    "print(df.describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da3d974c-44b9-4c3f-be02-a30514928514",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Exploration:\n",
      "                                                Data       Labels\n",
      "0  Path: cantaloupe.srv.cs.cmu.edu!magnesium.club...  alt.atheism\n",
      "1  Newsgroups: alt.atheism\\nPath: cantaloupe.srv....  alt.atheism\n",
      "2  Path: cantaloupe.srv.cs.cmu.edu!das-news.harva...  alt.atheism\n",
      "3  Path: cantaloupe.srv.cs.cmu.edu!magnesium.club...  alt.atheism\n",
      "4  Xref: cantaloupe.srv.cs.cmu.edu alt.atheism:53...  alt.atheism\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2000 entries, 0 to 1999\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   Data    2000 non-null   object\n",
      " 1   Labels  2000 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 31.4+ KB\n",
      "None\n",
      "                                                     Data       Labels\n",
      "count                                                2000         2000\n",
      "unique                                               2000           20\n",
      "top     Path: cantaloupe.srv.cs.cmu.edu!magnesium.club...  alt.atheism\n",
      "freq                                                    1          100\n"
     ]
    }
   ],
   "source": [
    "# Data Exploration and Preprocessing\n",
    "print(\"Data Exploration:\")\n",
    "print(df.head())\n",
    "print(df.info())\n",
    "print(df.describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8308d740-0665-4b7a-8277-325a2d7cec87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Data', 'Labels'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Print the column names\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "33ecc838-9858-4618-bae4-27ddcb4dadef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text:\n",
      "0    path: cantaloupe.srv.cs.cmu.edu!magnesium.club...\n",
      "1    newsgroups: alt.atheism\\npath: cantaloupe.srv....\n",
      "2    path: cantaloupe.srv.cs.cmu.edu!das-news.harva...\n",
      "3    path: cantaloupe.srv.cs.cmu.edu!magnesium.club...\n",
      "4    xref: cantaloupe.srv.cs.cmu.edu alt.atheism:53...\n",
      "Name: Data, dtype: object\n",
      "\n",
      "Text after converting to lowercase:\n",
      "0    path: cantaloupe.srv.cs.cmu.edu!magnesium.club...\n",
      "1    newsgroups: alt.atheism\\npath: cantaloupe.srv....\n",
      "2    path: cantaloupe.srv.cs.cmu.edu!das-news.harva...\n",
      "3    path: cantaloupe.srv.cs.cmu.edu!magnesium.club...\n",
      "4    xref: cantaloupe.srv.cs.cmu.edu alt.atheism:53...\n",
      "Name: Data, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Convert text data to lowercase\n",
    "print(\"Original Text:\")\n",
    "print(df['Data'].head())\n",
    "df['Data'] = df['Data'].apply(lambda x: x.lower())\n",
    "print(\"\\nText after converting to lowercase:\")\n",
    "print(df['Data'].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b50d31b3-4d25-4429-aab4-7d83cf5ba1e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Text after removing punctuation and special characters:\n",
      "0    path cantaloupesrvcscmuedumagnesiumclubcccmued...\n",
      "1    newsgroups altatheism\\npath cantaloupesrvcscmu...\n",
      "2    path cantaloupesrvcscmuedudasnewsharvardedunoc...\n",
      "3    path cantaloupesrvcscmuedumagnesiumclubcccmued...\n",
      "4    xref cantaloupesrvcscmuedu altatheism53485 tal...\n",
      "Name: Data, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Remove punctuation and special characters\n",
    "print(\"\\nText after removing punctuation and special characters:\")\n",
    "df['Data'] = df['Data'].apply(lambda x: ''.join(e for e in x if e.isalnum() or e.isspace()))\n",
    "print(df['Data'].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5e3f3ba1-625a-48a3-bc0b-c05b9d8d7d3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Text after tokenization:\n",
      "0    [path, cantaloupesrvcscmuedumagnesiumclubcccmu...\n",
      "1    [newsgroups, altatheism, path, cantaloupesrvcs...\n",
      "2    [path, cantaloupesrvcscmuedudasnewsharvardedun...\n",
      "3    [path, cantaloupesrvcscmuedumagnesiumclubcccmu...\n",
      "4    [xref, cantaloupesrvcscmuedu, altatheism53485,...\n",
      "Name: Data, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the text data\n",
    "print(\"\\nText after tokenization:\")\n",
    "df['Data'] = df['Data'].apply(lambda x: x.split())\n",
    "print(df['Data'].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2cc801b3-fc53-4b80-85b3-f25bf93afc04",
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\adhil/nltk_data'\n    - 'C:\\\\ProgramData\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\ProgramData\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\ProgramData\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\adhil\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\nltk\\corpus\\util.py:84\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 84\u001b[0m     root \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubdir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mzip_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords.zip/stopwords/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\adhil/nltk_data'\n    - 'C:\\\\ProgramData\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\ProgramData\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\ProgramData\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\adhil\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Remove stopwords\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m stopwords\n\u001b[1;32m----> 3\u001b[0m stop_words \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(stopwords\u001b[38;5;241m.\u001b[39mwords(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mText after removing stopwords:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mData\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mData\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: [word \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m x \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stop_words])\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\nltk\\corpus\\util.py:121\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLazyCorpusLoader object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 121\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__load()\n\u001b[0;32m    122\u001b[0m \u001b[38;5;66;03m# This looks circular, but its not, since __load() changes our\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;66;03m# __class__ to something new:\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\nltk\\corpus\\util.py:86\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     84\u001b[0m             root \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubdir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mzip_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     85\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n\u001b[1;32m---> 86\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m     88\u001b[0m \u001b[38;5;66;03m# Load the corpus.\u001b[39;00m\n\u001b[0;32m     89\u001b[0m corpus \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__reader_cls(root, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__kwargs)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\nltk\\corpus\\util.py:81\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 81\u001b[0m         root \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubdir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     83\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    581\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\adhil/nltk_data'\n    - 'C:\\\\ProgramData\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\ProgramData\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\ProgramData\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\adhil\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "# Remove stopwords\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "print(\"\\nText after removing stopwords:\")\n",
    "df['Data'] = df['Data'].apply(lambda x: [word for word in x if word not in stop_words])\n",
    "print(df['Data'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c130fe73-7e0c-4f04-9ccd-5735e56b3fb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\adhil\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "07f514cf-7026-4ce6-80dd-00655d29ce67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Text after removing stopwords:\n",
      "0    [path, cantaloupesrvcscmuedumagnesiumclubcccmu...\n",
      "1    [newsgroups, altatheism, path, cantaloupesrvcs...\n",
      "2    [path, cantaloupesrvcscmuedudasnewsharvardedun...\n",
      "3    [path, cantaloupesrvcscmuedumagnesiumclubcccmu...\n",
      "4    [xref, cantaloupesrvcscmuedu, altatheism53485,...\n",
      "Name: Data, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Remove stopwords\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "print(\"\\nText after removing stopwords:\")\n",
    "df['Data'] = df['Data'].apply(lambda x: [word for word in x if word not in stop_words])\n",
    "print(df['Data'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f3ae50b4-79b3-4fdf-a4d9-d1452aac7a25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Text after joining tokens back into strings:\n",
      "0    path cantaloupesrvcscmuedumagnesiumclubcccmued...\n",
      "1    newsgroups altatheism path cantaloupesrvcscmue...\n",
      "2    path cantaloupesrvcscmuedudasnewsharvardedunoc...\n",
      "3    path cantaloupesrvcscmuedumagnesiumclubcccmued...\n",
      "4    xref cantaloupesrvcscmuedu altatheism53485 tal...\n",
      "Name: Data, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Join the tokens back into strings\n",
    "print(\"\\nText after joining tokens back into strings:\")\n",
    "df['Data'] = df['Data'].apply(lambda x: ' '.join(x))\n",
    "print(df['Data'].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dde52083-3145-430e-a861-ca2bfcfff889",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (2000, 56437)\n",
      "y shape: (2000,)\n"
     ]
    }
   ],
   "source": [
    "# Feature Extraction using TF-IDF\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(df['Data'])\n",
    "y = df['Labels']\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"y shape:\", y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "79a5640e-ab08-464f-b2ee-62c4d5db5759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (1600, 56437)\n",
      "X_test shape: (400, 56437)\n",
      "y_train shape: (1600,)\n",
      "y_test shape: (400,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c4238b28-cd68-46d8-acfd-5b525cc78057",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.82\n",
      "Precision: 0.844747244324381\n",
      "Recall: 0.82\n",
      "F1-score: 0.8144501134827706\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes Model for Text Classification\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Precision:\", precision_score(y_test, y_pred, average='weighted'))\n",
    "print(\"Recall:\", recall_score(y_test, y_pred, average='weighted'))\n",
    "print(\"F1-score:\", f1_score(y_test, y_pred, average='weighted'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ca66225d-5e49-4171-bd13-edadaedfdb3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment Distribution:\n",
      "Sentiment\n",
      "Positive    1353\n",
      "Negative     584\n",
      "Neutral       63\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Sentiment Analysis\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "df['Sentiment'] = df['Data'].apply(lambda x: sia.polarity_scores(x)['compound'])\n",
    "df['Sentiment'] = df['Sentiment'].apply(lambda x: 'Positive' if x >= 0.05 else 'Negative' if x <= -0.05 else 'Neutral')\n",
    "\n",
    "print(\"Sentiment Distribution:\")\n",
    "print(df['Sentiment'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e6d8a2-69a7-4669-ba81-ddbd2f7440f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
