{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bea401bb-df6d-43f2-a5a9-2a4aeb6bed97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2492b97-20ca-451c-87c5-5c2da5969762",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import BaggingClassifier, AdaBoostClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87fee384-64c1-4f9a-9f42-c6ed45849261",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Glass dataset\n",
    "data = pd.read_excel('glass.xlsx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea897888-f8cf-4609-88a3-85b010382e9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Prepare a model for glass classification using Random Forest'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1e5ec926-1621-4b3d-85c1-dca25b1a2482",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Prepare a model for glass classification using Random Forest'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Check the column names\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "59efa6b3-e83e-415c-9a59-b4614bc893ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text\n",
      "0                                  Data Description:\n",
      "1                              RI : refractive index\n",
      "2  Na: Sodium (unit measurement: weight percent i...\n",
      "3                                      Mg: Magnesium\n",
      "4                                       AI: Aluminum\n"
     ]
    }
   ],
   "source": [
    "# Rename the column to something more manageable\n",
    "data.columns = ['text']\n",
    "\n",
    "# Print the first few rows of the DataFrame\n",
    "print(data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5bd3a546-19c9-4095-ab6d-e9546414a576",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data:\n",
      "                                                 text\n",
      "8                                          Ba: Barium\n",
      "16                                     5 --containers\n",
      "3                                       Mg: Magnesium\n",
      "13           2 --building_windows_non_float_processed\n",
      "15   4 --vehicle_windows_non_float_processed (none...\n",
      "\n",
      "Test Data:\n",
      "                                      text\n",
      "0                        Data Description:\n",
      "5                              Si: Silicon\n",
      "11  Type: Type of glass: (class attribute)\n",
      "1                    RI : refractive index\n"
     ]
    }
   ],
   "source": [
    "# Split the data into train and test sets\n",
    "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Print the first few rows of the train and test DataFrames\n",
    "print(\"Train Data:\")\n",
    "print(train_data.head())\n",
    "print(\"\\nTest Data:\")\n",
    "print(test_data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2d200da0-d34c-4028-a674-53dfd57e6ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since we don't have a typical dataset, we can't use a Random Forest classifier\n",
    "# Instead, we could try using a text classification model, such as a Naive Bayes classifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5d302f7f-2442-41ad-abf3-f6bd60c5474c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a CountVectorizer to convert the text data into a numerical format\n",
    "vectorizer = CountVectorizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "94bd3670-4d9f-4693-95c0-d5210b8287e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Remove rows with missing values\n",
    "data = data.dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0b5e56ee-221c-42a7-8bf9-602aa6cf015b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the column to something more manageable\n",
    "data.columns = ['text']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "98e80bb2-3841-4524-bef3-a6bea607b41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows with missing values\n",
    "data = data.dropna()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4bd55a3c-17a9-4498-ab11-293b7e37765f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are no NaN values in the text data.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Check if there are any NaN values left\n",
    "if data['text'].isnull().values.any():\n",
    "    print(\"There are NaN values in the text data.\")\n",
    "else:\n",
    "    print(\"There are no NaN values in the text data.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "28d1fbde-63d9-4493-9e6d-5be68acae204",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train and test sets\n",
    "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9d962d5a-63ec-472e-8529-e4a051cd0b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a CountVectorizer to convert the text data into a numerical format\n",
    "vectorizer = CountVectorizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "70271e97-e615-4848-8b53-4ce2adbc535e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (14, 37)\n",
      "X_test shape: (4, 37)\n"
     ]
    }
   ],
   "source": [
    " # Fit the vectorizer to the training data and transform both the training and test data\n",
    "X_train = vectorizer.fit_transform(train_data['text'])\n",
    "X_test = vectorizer.transform(test_data['text'])\n",
    "\n",
    "# Print the shape of the resulting matrices\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2506cc4d-46c1-4da8-82c7-6e992f40083a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (14, 37)\n",
      "X_test shape: (4, 37)\n",
      "Feature names:\n",
      "['10' 'ai' 'aluminum' 'are' 'as' 'attribute' 'attributes'\n",
      " 'building_windows_float_processed' 'building_windows_non_float_processed'\n",
      " 'ca' 'calcium' 'class' 'containers' 'corresponding' 'database' 'fe'\n",
      " 'glass' 'headlamps' 'in' 'iron' 'magnesium' 'measurement' 'mg' 'na'\n",
      " 'none' 'of' 'oxide' 'percent' 'potassium' 'sodium' 'tableware' 'this'\n",
      " 'type' 'unit' 'vehicle_windows_float_processed'\n",
      " 'vehicle_windows_non_float_processed' 'weight']\n"
     ]
    }
   ],
   "source": [
    "# Print the shape of the resulting matrices\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "\n",
    "# Print the feature names (i.e., the words in the vocabulary)\n",
    "print(\"Feature names:\")\n",
    "print(vectorizer.get_feature_names_out())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bc8eb172-d0f7-4eae-af4d-49e01ff78b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, BaggingClassifier, AdaBoostClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming data is your DataFrame\n",
    "X = data.iloc[:, :-1]  # features\n",
    "y = data.iloc[:, -1]   # target\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "90cceb1a-1e20-4993-971e-128164fb2f20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy: 1.0\n",
      "Bagging Accuracy: 1.000\n",
      "Boosting Accuracy: 1.000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, BaggingClassifier, AdaBoostClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the Glass dataset\n",
    "data = pd.read_excel('glass.xlsx')\n",
    "\n",
    "# Rename the column to something more manageable\n",
    "data.columns = ['text']\n",
    "\n",
    "# Remove rows with missing values\n",
    "data = data.dropna()\n",
    "\n",
    "# Create a target variable (this should be replaced with your actual target variable)\n",
    "data['target'] = np.random.randint(0, 2, size=len(data))  # Replace with your actual target variable\n",
    "\n",
    "# Create a CountVectorizer to convert the text data into a numerical format\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit the vectorizer to the data and transform it\n",
    "X = vectorizer.fit_transform(data['text'])\n",
    "\n",
    "# Define the target variable\n",
    "y = data['target']\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a Random Forest classifier\n",
    "rfc = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rfc.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the Random Forest classifier\n",
    "y_pred = rfc.predict(X_test)\n",
    "print(\"Random Forest Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "\n",
    "# Train a Bagging classifier\n",
    "bagging = BaggingClassifier(estimator=RandomForestClassifier(), n_estimators=10, random_state=42)\n",
    "bagging.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the Bagging classifier\n",
    "y_pred_bagging = bagging.predict(X_test)\n",
    "bagging_accuracy = accuracy_score(y_test, y_pred_bagging)\n",
    "print(f\"Bagging Accuracy: {bagging_accuracy:.3f}\")\n",
    "\n",
    "# Train a Boosting classifier\n",
    "boosting = AdaBoostClassifier(RandomForestClassifier(), n_estimators=10, random_state=42, learning_rate=0.1, algorithm='SAMME')\n",
    "boosting.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the Boosting classifier\n",
    "y_pred_boosting = boosting.predict(X_test)\n",
    "boosting_accuracy = accuracy_score(y_test, y_pred_boosting)\n",
    "print(f\"Boosting Accuracy: {boosting_accuracy:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3329ec6-749c-4451-a968-c227cc04637d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Bagging and Boosting Methods\n",
    "\n",
    "Bagging (Bootstrap Aggregating)\n",
    "\n",
    "Bagging is an ensemble learning method that combines multiple instances of a model trained on different subsets of the training data. The subsets are created by sampling the training data with replacement, which is known as bootstrapping.\n",
    "\n",
    "Here's how bagging works:\n",
    "\n",
    "1. Create multiple subsets of the training data using bootstrapping.\n",
    "2. Train a model on each subset.\n",
    "3. Make predictions on the test data using each model.\n",
    "4. Combine the predictions using voting or averaging.\n",
    "\n",
    "Bagging helps to:\n",
    "\n",
    "- Reduce overfitting by averaging out the errors of individual models.\n",
    "- Improve the stability of the model by reducing the effect of outliers.\n",
    "\n",
    "Boosting\n",
    "\n",
    "Boosting is an ensemble learning method that combines multiple weak models to create a strong model. The weak models are trained sequentially, with each model trying to correct the errors of the previous model.\n",
    "\n",
    "Here's how boosting works:\n",
    "\n",
    "1. Train a weak model on the training data.\n",
    "2. Calculate the errors of the weak model.\n",
    "3. Train a new weak model on the training data, with a focus on correcting the errors of the previous model.\n",
    "4. Repeat steps 2-3 until a specified number of models have been trained.\n",
    "5. Combine the predictions of all the models using weighted voting.\n",
    "\n",
    "Boosting helps to:\n",
    "\n",
    "- Improve the accuracy of the model by focusing on the most difficult cases.\n",
    "- Reduce overfitting by using a combination of weak models.\n",
    "\n",
    "\n",
    "    \n",
    "Key differences between Bagging and Boosting\n",
    "\n",
    "- Purpose: Bagging is primarily used to reduce overfitting, while boosting is used to improve the accuracy of the model.\n",
    "- Model combination: Bagging combines models using voting or averaging, while boosting combines models using weighted voting.\n",
    "- Model training: Bagging trains models in parallel, while boosting trains models sequentially.\n",
    "- Error correction: Bagging does not focus on correcting errors, while boosting focuses on correcting the errors of previous models.\n",
    "\n",
    "Handling Imbalance in the Data\n",
    "\n",
    "Imbalanced data refers to a situation where one class has a significantly larger number of instances than the other classes. This can cause problems for machine learning models, as they may become biased towards the majority class.\n",
    "\n",
    "Here are some techniques for handling imbalanced data:\n",
    "\n",
    " 1. Oversampling the minority class\n",
    "\n",
    "- Create additional instances of the minority class by replicating existing instances or generating new instances using techniques such as SMOTE (Synthetic Minority Over-sampling Technique).\n",
    "\n",
    "2. Undersampling the majority class\n",
    "\n",
    "- Reduce the number of instances in the majority class by randomly removing instances or using techniques such as Tomek links.\n",
    "\n",
    "3. Class weighting\n",
    "\n",
    "- Assign different weights to different classes during training, with the minority class receiving a higher weight.\n",
    "\n",
    "4. Anomaly detection\n",
    "\n",
    "- Treat the minority class as anomalies and use anomaly detection techniques such as One-class SVM or Local Outlier Factor (LOF).\n",
    "\n",
    "5. Ensemble methods\n",
    "\n",
    "- Use ensemble methods such as bagging or boosting, which can help to reduce the effect of class imbalance.\n",
    "\n",
    "6. Metric selection\n",
    "\n",
    "- Use metrics such as precision, recall, and F1-score, which are more suitable for imbalanced data than accuracy.\n",
    "\n",
    "By using these techniques, you can help to improve the performance of your machine learning models on imbalanced data.                        \n",
    "\n",
    "                        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
